import os
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"
import math
import h5py
from sklearn.metrics import roc_auc_score, roc_curve
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
#import matplotlib.pyplot as plt
#import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn import preprocessing, model_selection, metrics
import lightgbm as lgb
from sklearn.preprocessing import LabelEncoder
import gc
from keras.models import load_model
import tensorflow as tf
from keras.layers import * #Input, Embedding, Dense,Flatten, merge,Activation
from keras.models import Model
from keras.regularizers import l2 as l2_reg
import itertools
import keras
from keras.optimizers import *

from keras.models import Sequential
from keras.layers import *

import keras.backend as K
from keras.utils.generic_utils import get_custom_objects

def Swish(x):
    return (K.sigmoid(x) * x)

class Swish_Class(Activation):
    
    def __init__(self, activation, **kwargs):
        super(Swish_Class, self).__init__(activation, **kwargs)
        self.__name__ = 'SWISH'
get_custom_objects().update({'Swish': Swish_Class(Swish)})
#import plotly.offline as py
#py.init_notebook_mode(connected=True)
#import plotly.graph_objs as go
#import plotly.tools as tls

pd.options.mode.chained_assignment = None
pd.options.display.max_columns = 9999
def KerasFM(max_features,dense_input,max_features2,K=7,K2=7,solver=Adam(lr=0.01, amsgrad=True),l2=0.00,l2_fm = 0.00):
    inputs = []
    id_inputs = []
    emb1_list = []
    flatten_layers=[]
    fm_layers = []
    columns = range(len(max_features))
    for c in columns:
        inputs_c = Input(shape=(1,), dtype='int32',name = 'input_%s'%c)
        num_c = max_features[c]
        print(num_c)
        embed_c = Embedding(
                        num_c,
                        K,
                        input_length=1,
                        name = 'embed_%s'%c
                        )(inputs_c)
        emb1_list.append(embed_c)
        flatten_c = Flatten()(embed_c)
        inputs.append(inputs_c)
        flatten_layers.append(flatten_c)
    input_dense = Input(shape=(len(dense_input),), dtype='float32',name = 'dense_feature')
    inputs.append((input_dense))
    dense_1 = Dense(1,activation='sigmoid',kernel_regularizer=keras.regularizers.l2(1e-5))(Dense(K*2,activation='Swish')(Dense(K*16,activation='Swish')(BatchNormalization(axis=1)(input_dense))))

    columns = range(len(max_features2))
    an_inputs = []
    for c in columns:
        inputs_c = Input(shape=(1,), dtype='int32',name = 'input2_%s'%c)
        num_c = max_features2[c]
        an_inputs.append(inputs_c)
        inputs.append(inputs_c)

    for i in range(0,len(columns)):
        for j in range(i+1,len(columns)):
            num_i = max_features2[i]
            num_j = max_features2[j]
            embed_i = Embedding(num_i,K2,input_length=1,name = '%d_%d'%(i,j))(an_inputs[i])
            embed_j = Embedding(num_j,K2,input_length=1,name = '%d_%d'%(j,i))(an_inputs[j])
            emb1_i = add([Flatten()(emb1_list[i]),Flatten()(embed_i)])
            emb1_j = add([Flatten()(emb1_list[j]),Flatten()(embed_j)])
            dot_value = dot([emb1_i,emb1_j],axes=-1)
            fm_layers.append(dot_value)
        dense_k = Dense(K,activation='Swish',kernel_regularizer=keras.regularizers.l2(1e-5))(Dense(K*2,activation='Swish')(Dense(K*16,activation='Swish')(BatchNormalization(axis=1)(input_dense))))
        dot_value = dot([emb1_i,dense_k],axes=-1)
        fm_layers.append(dot_value)
    for c in columns:
        num_c = max_features[c]
        embed_c = Embedding(
                        num_c,
                        1,
                        input_length=1,
                        name = 'linear_%s'%c
                        )(inputs[c])
        flatten_c = Flatten()(embed_c)
        fm_layers.append(flatten_c)
    fm_layers.append(dense_1) 
    flatten = average(fm_layers)
    #flatten = add(fm_layers)
    outputs = Dense(1,activation='sigmoid',name='outputs',kernel_regularizer=keras.regularizers.l2(1e-5))(flatten)
    model = Model(input=inputs, output=outputs)
    model.compile(optimizer=solver,loss= 'binary_crossentropy')
    model.summary()
    return model

id_list = ['2', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39']
dense_list=['1', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13']
feat_col = []
dtypes = {}
for col in id_list:
    dtypes[col] = 'int32'
for col in dense_list:
    dtypes[col] = 'float32'
data = pd.read_csv("train.all.csv",sep=",",header=0,dtype=dtypes)
test_data = pd.read_csv("test.all.csv",sep=",",header=0,dtype=dtypes)
data = data.sample(frac=1.0,random_state=2029)
for feat in data.columns:
    if '0' == feat:continue
    feat_col.append(feat)
print(id_list)
print(dense_list)
print(data[id_list].max())

for feat in id_list:
    size = int(data[feat].value_counts().shape[0])
    if size > 10000:
        size = int(math.sqrt(size)+1)
    print("%s size %d" %(feat,size))
    data[feat] = data[feat]%size
    test_data[feat] = test_data[feat]%size
max_features = data[id_list].max() + 1
id_list2=[]
for feat in id_list:
    size = int(data[feat].value_counts().shape[0])
    if size > 10000:
        size = int(math.sqrt(size)+2)
    print("%s size %d" %(feat,size))
    new_name = "%s_2"%(feat)
    id_list2.append(new_name)
    data[new_name] = data[feat]%size
    test_data[new_name] = test_data[feat]%size
max_features2 = data[id_list2].max() + 2
print(id_list2)

for feat in id_list2:
    feat_col.append(feat)

train_len = int(len(data)*0.99)
X_train, X_valid = data[feat_col][:train_len], data[feat_col][train_len:]
y_train, y_valid = data['0'][:train_len], data['0'][train_len:]

train_input = []
valid_input = []
test_input = []
#print(test_data)
for col in id_list:
    train_input.append(X_train[col])
    valid_input.append(X_valid[col])
    test_input.append(test_data[col])
train_input.append(X_train[dense_list])
valid_input.append(X_valid[dense_list])
test_input.append(test_data[dense_list])

for col in id_list2:
    train_input.append(X_train[col])
    valid_input.append(X_valid[col])
    test_input.append(test_data[col])

ck = keras.callbacks.ModelCheckpoint("best.model", monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='auto', period=1)
es = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=1, verbose=0, mode='auto', baseline=None, restore_best_weights=True)
model = KerasFM(max_features,dense_list,max_features2)
#model.load_weights('./best.model')
model.fit(train_input, y_train, class_weight={1:1,0:1},batch_size=100000,nb_epoch=30,validation_data=(valid_input,y_valid),callbacks=[ck,es])
#model.load_weights('./best.model')
#model.compile(optimizer=Nadam(1e-3),loss= 'binary_crossentropy')
#model.fit(train_input, y_train, class_weight={1:1,0:1},batch_size=50000,nb_epoch=10,validation_data=(valid_input,y_valid),callbacks=[ck,es])
#print(data)
sub_df = pd.DataFrame({"Id":test_data["0"].values})
sub_df["Predicted"] = model.predict(test_input,batch_size=10000)
sub_df.to_csv("ffm.csv", index=False)

